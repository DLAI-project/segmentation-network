\documentclass[12pt, a4paper]{scrartcl}

\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{multicol}
\usepackage{ dsfont }
\usepackage[dvipsnames]{xcolor}  %Per evidenziare

%Questo genera un Lorem Ipsum
\usepackage{lipsum}

%Questo usa i colori
\usepackage{xcolor}


\usepackage{movie15}
%Link nelle cose
\usepackage[hidelinks]{hyperref}

%Questo � per scrivere in italiano
%\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} %questo prima era italian 

%Roba di matematica
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mhchem}

%Per mettere le foto
\usepackage{graphicx} 
\usepackage{movie15}
\usepackage{hyperref}
\usepackage{wrapfig}

%Per controllare i float (figure)
\usepackage{float}

%Questo � per i margini di pagina
\usepackage[margin=2cm,left=2.5cm,includefoot]{geometry}

%Questo per varie figure vicine
\usepackage{lipsum}
\usepackage{subfig}
\usepackage{subfigure}



%Questo � per i numeri e lo stile figo
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[R]{ \thepage\ }
\renewcommand{\headrulewidth}{1pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[R]{ \thepage\ }
\renewcommand{\headrulewidth}{1pt}
\renewcommand*\listtablename{Elenco Tabelle}
\renewcommand*\contentsname{Indice}

%Questo per inserire il codice python
\usepackage{pythontex} 
\usepackage{minted} %codice già inserito


\begin{document}    %Aperto ogni volta e chiuso con \end{asdf}

\begin{title}	%Inizia il Titolo
%	 \begin{center}	%centra le cose
% 		\line(1,0){300}\\     	%linee con [spessore]{lunghezza}
% 		[7mm]				%questo indica quanto spazio tra una riga e l'altra
		
		\LARGE{\bfseries  U-Net: Segmentation of Biomedical Images
 }\\
		[3mm]
		\small{\bfseries Implementation and contributions to the U-Net network, a deep convolutional encoder-decoder trained to perform semantic segmentation on biomedical images, in order to both distinguish whether there is a disease and localize the abnormal area.}\\     % \\ manda a capo
% 		\line(2,0){300}\\
%		[1cm]
%		\end{center}
			\end{title}
		\begin{center}
		\textbf{Flavio Giorgi\footnotemark[1], Filippo Liguori\footnotemark[2] \& Valerio Mannucci\footnotemark[2]}\\
		\footnotetext[1]{Sapienza Università di Roma, Dipartimento di Informatica, Master Student}
		\footnotetext[2]{Sapienza Università di Roma, Dipartimento di Fisica, Master Student}
		\textsc{\small Professor: Emanuele Rodolà}\\
		\textsc{\small Course: Deep Learning and Applied Artificial Intelligence}\\
		[1cm]
	 \end{center}
	 %\tableofcontents%indice ma non sta bene qui

\begin{abstract}
   This report describes the work carried out as final project of the course \textit{Deep Learning and Applied Artificial Intelligence}. We are going to introduce the semantic image segmentation problem and its importance in biomedical imaging, and describe the features and performance of UNet, the convolutional network we studied and implemented to address semantic segmentation tasks.
\end{abstract}
\iffalse
\begin{multicols}{2}
\fi
\section*{Introduction}
`\textit{Our ability to measure and alter biology far surpasses our ability to actually understand it}', writes Ed Regis in his renowned book \textit{What is life?}, citing Brendan Frey, co-founder of the Canadian company \textit{Deep Genomics}. We indeed know very little about the complexity of biological processes, compared to the accuracy we have achieved in observing and even engineering them. Imaging, synthesis and sequencing techniques can be used in perspective to build complex genetic circuits that replicate pre-programmed metabolic processes \cite{VoigtRNA}, and to advance the diagnosis and treatment of many diseases poorly known today. State of the art is still far from this goal, but a lot of cutting edge research is being dedicated to integrating Engineering, Biology, Physics and Computer Science to better understand biological processes and analyze biomedical data.\\After briefly introducing both the \textbf{segmentation problem} in general, with a focus on the segmentation of biomedical images and its relevance in the first section, in the second section of the review, we will give a deeper insight into our \textbf{work environment} and its features. In the third section, we will describe the \textbf{dataset} used to train the network, the pre-processing steps we performed to uniform and analyze it, and the purpose and features of the data augmentation we implemented. In the fourth section, we will describe the \textbf{architecture} of the network, characterizing all the layers. As already mentioned, our network is based on U-Net, a fully connected convolutional network developed at the Computer Science Department of the University of Freiburg, Germany, but we will stress our personal contributions. Before ultimately presenting the \textbf{performances} of the network on two different datasets and summarizing the conclusions and outlook of the work recapped in this report, in the fifth section we will delineate the features of the scheduling of optimizations parameters and tuning of the \textbf{hyperparameters}.


\section{Theoretical Context}
\subsection{Segmentation Problem}
The field of Computer Vision has largely benefited from the growing complexity of deep learning algorithms that have been developed in the last lustra. U-Net, whose architecture we have implemented, expanded and studied, exploits the potential of a convolutional encoder-decoder model to perform \textbf{semantic segmentation}. There are various levels of granularity in which the computers can gain an understanding of images. For each of these levels there is a problem defined in the Computer Vision domain. Starting from a \textit{coarse grained} down to a more \textbf{fine grained} understanding, we can define the following list of segmentation problems, and describe each of them:
\begin{enumerate}
     \item \textbf{Image Classification}
    \item \textbf{Classification with Localization}
    \item \textbf{Object Detection}
    \item \textbf{Semantic Segmentation}
    \item \textbf{Instance Segmentation}
\end{enumerate}

\textit{Image classification} is the most fundamental problem in Computer Vision, in which the algorithm takes as input a single image containing only one object, and is expected to output a discrete label that most accurately describes the image. One more step is needed if the algorithm is expected to perform \textit{classification with localization}, where along with classification the object should be localized in a frame of reference defined considering the image boundary. \textit{Object detection} takes image classification to the next level. It can be performed on images that contain more than one object, and classification and localization must be obtained for all of them. Further sophistication is introduced by \textit{semantic segmentation}, namely labelling each pixel of an image with a corresponding class of what is being represented. Unlike the previous ones, the output of a semantic segmentation algorithm is another image, typically the same size as the input, in which each pixel is classified to a particular class. The highest level of sophistication is obtained through \textit{instance segmentation}, wherein along with semantic segmentation, the algorithm is expected to classify each instance of a class separately. For example, semantic segmentation of an image of a population of cells (being them cancer cells, differentiating stem cells or a culture in vitro), outputs an image the same size of the original, with the \textbf{masks} of the cells, as we will see in the next sections. Instance segmentation would further label each of the masks, for example outputting an image of the masks of the cells, each coloured in a different way, differentiating between the instances of the `\textit{cells class}'.
\subsection{Segmentation of biomedical images}
Humans possess a great variety of sensory systems that diagnosticians can exploit to identify whether a certain medical disease is present in a patient or not. Automating this process in a robust and accountable way is still a very open challenge today. When a human observer views a scene, his or her visual system segments the scene semantically, and this feature can be used widely for diagnostic purposes. Automated visual analysis of biomedical images can prove itself as an extremely powerful tool to speed up the diagnostic process.
\begin{wrapfigure}{l}{5.5cm}
\caption{Pulmonary nodule}\label{nodule}
\includegraphics[width=5 cm]{Foto_polmoni2.jpg}
\end{wrapfigure}
By marking the desired type of cells with a known dye, the cells nuclei can be spotted and photographed. Subsequently identifying the nuclei through segmentation, allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, we can understand the underlying biological processes at work.\\
In this section we briefly present as an example an image segmentation problem of great importance in biomedical data analysis, namely `\textit{pulmonary nodule detection and segmentation for early diagnosis of lung cancer}'. 
\textbf{Lung nodules} are small masses of tissue in the lung, that appear as round, white shadows on a \textbf{computerized-tomography} scan (CT)\footnote{An imaging procedure that uses computer-processed combinations of many X-ray measurements taken from different angles to produce cross-sectional (\textbf{tomographic}) images (\textit{virtual `slices'}) of the organ.}, as we see in figure \ref{nodule}. Lung nodules are usually 5 to 30 millimetres in size. A larger lung nodule, such as one that's 30 millimeters or larger, is very likely to be cancerous, and segmentation of CT scans, as the one in figure \ref{nodule} can identify these regions and automatically diagnose the presence of nodules. In section 6 we'll see another application of our semantic segmentation network in biomedical research. We're not going to focus on other applications of segmentation in this report, but its features are widely exploited in other relevant problems, such as autonomous vehicles driving, geological sensing and precision agriculture.

%\iffalse
%Lung cancer in the United States accounts for 30\% of all cancer-related deaths, resulting in over 160,000 deaths per year, which is more than the annual deaths for colon, breast, prostate, ovarian and pancreatic cancers combined. The survival of lung cancer is strongly \textbf{diagnosis-dependent}.
%\fi

%\begin{figure}[H]
%\begin{center}
%    \includegraphics[width=13cm]{Foto_polmoni.jpg}
%    \caption{Pulmonary nodule detection with computerized-tomography for early diagnosis of lung cancer}
%\end{center}
%\end{figure}
%As an example of how an instance segmentation approach could impact lung cancer diagnosis, the work cited in the bibliography\cite{lungs} presents a systematic review of techniques for the 3D automatic detection of pulmonary nodules in \textbf{computerized-tomography} (CT) images. A CT scan is a medical imaging procedure that uses computer-processed combinations of many X-ray measurements taken from different angles to produce cross-sectional (\textbf{tomographic}) images (virtual `slices') of specific areas of a scanned object, allowing the user to see inside the object without cutting.\\
%The article are analyzes the latest technology being used for the development of computational diagnostic tools to assist in the acquisition\footnote{For example the cited CT scan}, storage and, mainly, processing and analysis of the biomedical data. Lung nodules are small masses of tissue in the lung, that appear as round, white shadows on a CT scan, as we see in the picture above. Lung nodules are usually 5 to 30 millimeters in size. A larger lung nodule, such as one that's 30 millimeters or larger, is very likely to be cancerous, and segmentation of CT scans can identify these regions and automatically diagnose the presence of nodules. We'll see another application of our semantic segmentation network in biomedical research in the last section. We're not going to focus on other applications of segmentation in this report, but its features are widely exploited in other relevant problems, such as autonomous vehicles, geological sensing and precision agriculture.


\section{Work Environment}
The network has been built on Google Colaboratory, a platform that enables us to execute code directly on the Cloud, whose interactivity is favoured by the \textit{Jupyter Notebooks style}. The features of Colab that make it useful to build a convolutional network, are the \textit{interaction with the Drive} and the \textit{hardware} Google provides to Colab users. We can \textit{mount} Google Drive on our Colab notebook, and all the documents saved in \textit{My Drive} are mounted in a directory called \textit{my-drive}, in the root of our file system. This feature proves to be useful when dealing with a large dataset to train the network.\\The second feature that made Colab essential in carrying out this project was the \textbf{Google GPUs}. We exploited the deep learning framework \textit{PyTorch}, considered by many sources the leading one for research. There has been much recent research dedicated to studying the importance of the hardware on the training speed of deep neural networks, and some works by \textit{Indigo AI} showed that GPUs can be 250 times faster than CPUs. Google has developed ulterior hardware in this context, releasing the \textit{Application-Specific Integrated Circuit} \textbf{Tensor Processing Unit}, that goes beyond the GPU technology and was made available for third party use in 2018. The TPU was developed specifically for neural network machine learning, but is still too specific for Google's own TensorFlow software\footnote{Arguably the second leading framework for research in Deep Learning, and still the leading one in industry.}. Despite these considerations, the GPU Cloud made available by Google to Colab users, was a powerful hardware for our purposes and has enabled us with enough speed in training our network.

\section{Dataset Analisys}
In order to train our network, we downloaded the nuclei images provided on the Kaggle website, as dataset for the \textit{2018 Data Science Bowl: Find the nuclei in divergent images to advance medical discovery}\cite{dataset}. As explained on the website in bibliography, the dataset contained 670 segmented nuclei images. Since the images were acquired under a variety of conditions, namely different tissues and cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset diversity was designed purposely to challenge an algorithm's ability to generalize across these variations. Below, we give an example of the diversity of the images present in the dataset.

\begin{figure}[H]
    \centering
    \subfigure[Instance 1 of 2018 DSB]{\includegraphics[width=5.5cm]{dataset1.png}} 
    \subfigure[Instance 2 of 2018 DSB]{\includegraphics[width=5.5cm]{dataset2.png}} 
    \subfigure[Instance 3 of 2018 DSB]{\includegraphics[width=5.5cm]{dataset3.png}}
    \subfigure[Instance 4 of 2018 DSB]{\includegraphics[width=5.5cm]{dataset4.png}}
\end{figure}

\iffalse
\begin{figure}[H]
    \centering
    \subfloat[Instance 1 of 2018 DSB]{{\includegraphics[width=5cm]{dataset1.png}}}
    \qquad
    \subfloat[Instance 2 of 2018 DSB]{{\includegraphics[width=5cm]{dataset2.png}}}
    \qquad
    \subfloat[Instance 3 of 2018 DSB]{{\includegraphics[width=5cm]{dataset3.png}}}
    \qquad
    \subfloat[Instance 4 of 2018 DSB]{{\includegraphics[width=5cm]{dataset4.png}}}
\end{figure}
\fi

As mentioned, the dataset contained 670 couples of images, representing the \textit{instance files} and the corresponding \textit{mask}, or target image, namely the \textbf{ground truth} of the segmentation problem. The mask can be obtained by manual or automatic segmentation, but is supposed to be a very reliable segmentation of the instance. The first preprocessing step we had to perform on the dataset was \textbf{standardization}. The shapes of the images, being taken in different environment, were very variable. We had to uniform their shape to the optimal for our network, that we identified as $256\times256$. We had to act differently on the images bigger or smaller than the desired dimension.
\begin{itemize}
    \item Images bigger than $256\times256$ were sliced;
    \item Images smaller than $256\times256$ were simmetrically padded to reach the desired dimension.
\end{itemize}

\subsection{Data Augmentation}
Convolutional neural networks have been subject of research and developement for almost 40 years, but their popularity has not risen until lately due to the limited size of available datasets to train them. 
In June 2017, Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton published the breakthrough \textit{ImageNet Classification with Deep Convolutional Neural Networks}, describing `supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images.' Since then, even larger and deeper networks have been trained, due to even larger datesets. We know that the dataset is therefore essential in the learning process, and we performed data augmentation on the data presented in the last section, for two main reasons:
\begin{itemize}
    \item First, there is very little training data available for the nuclei segmentation task, and with augmentation transformations we can obtain a lot of pictures from a single one;
    \item Elastic deformations allow the network to learn invariance to such deformations, without the need to see these transformations in the \textit{image corpus}. This is particularly important in biomedical segmentation, since realistic deformations of the soft biological matter can be simulated efficiently.
\end{itemize}
Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 
microscopical images especially random \textbf{elastic deformations} of the training samples seem to be the key to train a segmentation network with very few annotated images, as shown in \cite{Article}.\\
We defined the method \texttt{transform()}, and randomly applied various transformations, comprising \textbf{horizontal} and \textbf{vertical flips}, \textbf{random rotations} and \textbf{regions erasing}, alongside \textbf{elastic transformations}\footnote{Applying only elastic deformation would have made the data augmentation process too slow.}, in order to augment the dataset and many more pictures, starting from the initial 670. Every time the model is trained, the dataset is therefore changed.

\iffalse
Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 
microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elas- tic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpola- tion. Drop-out layers at the end of the contracting path perform further implicit data augmentation.
Perchè avevamo bisogno di almeno 60k immagini di dataset?
Che trasformazioni abbiamo fatto e perchè randomicamente (deformation era troppo lenta)
\fi
\newpage
\section{Architecture of the network}
The architecture we inspired to is shown below. U-net is one of the most powerful architectures to perform biomedical images segmentation. We use it as a blueprint to build our own architecture exploiting the u-shape and the concatenation between encoder and decoder features.
\begin{figure}[H]
\begin{center}
    \includegraphics[width=13cm]{Network.png}
    \caption{Architecture of the U-Net Network}\label{arch}
\end{center}
\end{figure}
In particular, we chose for the contraction path a network used to perform images classification: the VGG-16\_bn network.
We decided to use it due to the possibility to download pre-trained weights and to switch on and off the batch-norm layers.
We performed two modifications to adapt VGG to our needs. We removed the fully connected layers and the softmax layer. Moreover, we slightly changed the width and height of the images. 
\begin{figure}[]
\begin{center}
    \includegraphics[width=13cm]{sd-Page-2 (2).png}
    \caption{Our model architecture}\label{arch}
\end{center}
\end{figure}

As we can see, it consists of a \textbf{contracting part} (on the left) and an \textbf{expansive part} (right side). The contracting part consists of two consecutive $3\times3$ unpadded convolution layers, each \textit{activated} by a rectified linear unit (\textbf{ReLU}), followed by a $2\times2$ max pooling operation with stride $2$. These layers achieve \textbf{downsampling}. At each downsampling step the number of feature channels \cite{features} is doubled. Every step in the expansive part consists of an upsampling of the feature map followed by a $2\times2$ convolution (denoted in \cite{Article} as “\textbf{up-convolution}”) that halves the number of feature channels. Each feature map of the expansive part is concatenated with the corresponding feature map (\textbf{cropped}) of the contractive part. Each concatenation is followed by two $3\times3$ convolutions, each again activated by a ReLU. The cropping in the concatenation step is necessary because border pixels are lost in every convolution layer. To obtain the output segmentation map, a final $1\times1$ convolution is used to map each \textbf{feature map} to the desired number of classes. We will better analyze these variables in the next subsection, when discussing the \textit{loss function} computation upon training. One of the changes we applied to the architecture shown in figure \ref{arch} was to add 4 convolutional layers, two after the last downsampling and two before the first upsampling. The network has therefore a total of 27 convolutional layers. We thought of adding some \textbf{dropout} layers, but since our model is not prone to overfitting, we discarded this idea.
Our model has a couple of implementation options that can be set before training. First, the user can `switch on' the \textbf{batch normalization}, that as we know makes every training example interact with the other examples of the minibatch. Despite beneficial properties of batch-norm, including the enhanced stability of gradient descent and speed of training, we observed a worsening in the performance of the network, due to decreased contrast particularly at the border of the pictures. As common when making deep learning models available to third party users, our model can be used as \textit{pretrained}. Its weights can then be frozen or it could be further trained to achieve better performance. The \textit{contracting part} of our model, is actually the encoder \textbf{vgg16b}, that we have used just this way, importing the pretrained model. Actually we removed the linear layers from the said model, because it had been trained to perform classification on the ImageNet dataset, consisting of images belonging to 1000 different classes. The linear layers were therefore added for this specific purpose, and were not useful to achieve semantic segmentation, whose output is an image just like the input.
\subsection{Training: loss and variables}
The first step in building the trainer, and therefore solve for the parameters ${\Theta_i}$ that best approximate the Multi-Layer Perceptron's function, is computing the loss. In our network, the loss function is computed by a \textbf{pixel-wise binary cross entropy with logit} :
\begin{equation}
    \mathcal{L}_p(y)= -\frac{1}{N} \sum^{N}_{i=1} y_i log(\sigma (x_i))+(1-y_i)log(1-\sigma (x_i))
\end{equation}
where $y_i$ represents the value of the i-th pixel of the ground truth mask, $\sigma (x_i)$ is the sigmoid function applied to the output of the model.

\begin{wrapfigure}{r}{5.5cm}
\caption{Visualization of the Dice Loss}\label{Dice}
\includegraphics[width=5 cm]{Dice.png}
\end{wrapfigure} 
\par
This is a different approach compared with the authors loss because they use a two channel output model and of course the soft-max loss.
Before identifying the BCE as the optimal loss function, we experimented training with \texttt{DiceLoss}, simply implemented as the ratio of twice the overlap of the generated mask and the original, over the sum of the two masks, as shown in picture \ref{Dice}. We discarded due to the late convergence, and because the predictions of the masks looked less reliable than those obtained with BCE. 
We quantified the reliability of the predictions by using the library \texttt{sklearn.metrics} function \texttt{f1\_score}. It is defined exactly like the Dice Loss; it outputs a real number between 0 and 1, where 1 quantifies the highest possible accuracy.
The code we wrote to declare the trainer is:
\inputminted{python}{trainer.py}
where we used our implemented class \texttt{Trainer}, whose objects are:
\begin{itemize}
    \item \texttt{optimizer}: algorithm to perform \textbf{backpropagation}, in our case \textit{SDG};
    \item \texttt{loss}: loss function of the model;
    \item \texttt{model}: all the layers of the deep network described in section 4;
    \item \texttt{train\_dataloader}: loader of the training set of images;
    \item \texttt{test\_dataloader}: loader of the validation set of images, to spot eventual overfitting;
    \item \texttt{epochs}: number of training epochs;
    \item \texttt{device}: hardware supporting the training;
    \item \texttt{scheduler}: \textbf{learning rate scheduler}, that could be set as \textit{Cyclic} or \textit{WarmRestart}, as will be better explained in the next section.
\end{itemize}
\iffalse
In deep networks with many convolutional layers, a good initialization of the weight map is extremely important and therefore we... \textbf{DA COMPLETARE}
\fi
\newpage
\section{Optimal hyperparameters tuning}
After building the architecture of the network, the next step was optimizing the learning hyperparameters, namely the so called \textbf{linesearch algorithm}. We imposed two different schedulers to the \textbf{learning rate}, \textit{WarmRestart} and \textit{Cyclic}, in order to find the optimal parameters for the gradient descent algorithm. We split the dataset in training and validation sets, and performed training for 30 epochs, using various networks with different schedulers, and varying the \textit{maximum learning rate} of the scheduler. As suggested in \cite{Article}, we used a high \textbf{momentum} (0.9) such that a large number of the previously seen training samples determine the update in the current optimization step. We show below the loss function during validation and training, and the f1 score as a function of the epochs, for the optimal network. 

\iffalse
\begin{figure}[H]
    \centering
    \subfloat[]{{\includegraphics[width=12cm]{TrainingLossesOK.png}}}
    \qquad
    \subfloat[]{{\includegraphics[width=12cm]{ValidationLossesOK.png}}}
\end{figure}
\fi

\begin{figure}[H]
    \centering
    \subfigure[]{\includegraphics[width=11.6cm]{batchsize.png}} 
    \subfigure[]{\includegraphics[width=11.6cm]{bestnet.png}} 
\end{figure}
As we see from the graphics, the optimal network, namely those that reach the minimum of the loss within the least epochs, has a mini batch size of 16. We observed a better performance with scheduler \textit{WarmRestart}, with maximum learning rate $0.4$. We used it to solve a novel biomedical segmentation problem, as described in section 6.

\section{Performance of the network: Results}
Once we identified the optimal neural network, we used it to segment different datasets from the one we used to train the network. We present in this section two examples of the performance of the network on these datasets.
\subsection{Differentiating neural stem cells}
As a first application of our network, we segmented an image provided by the laboratory of \textit{IIT@Sapienza} Center for Life Nano Science. The picture was taken with an epifluorescent microscope, and showed a population of fluorescing Neural Stem Cells differentiating and evolving \textit{in vitro}. The dimensions of the image were $4704\times6240$. As shown in figure \ref{imgbig}, the first step was slicing it in order to eliminate the borders on the bottom and on the right, owing to the acquisition method.  
\begin{figure}[H]%
 \centering
 \subfloat{\includegraphics[width = 7.2cm]{imgborder.png}}%
 \subfloat{\includegraphics[width = 7.2cm]{imgcrop.png}}\\
 \caption{On the left the original image, on the right the cropped one}\label{imgbig}
\end{figure}
 \noindent
Since the neural network can only perform segmentation on images whose dimensions are $256\times256$, we had to slice the big picture on the right of figure \ref{imgbig} and then give the slices as input to the network. To avoid major \textit{border} segmentation errors, we split the big picture in smaller tiles of dimension $128\times128$, applied \textbf{reflect padding} to them and obtaining $256\times256$ images, and ultimately sliced the inner $128\times128$ of the masks obtained as output of the network.
%\begin{figure}[H]%
% \centering
% \subfloat{\includegraphics[width = 6cm]{TileR1C1.png}}%
% \subfloat{\includegraphics[width = 6cm]{PaddedTileR1C1.png}}\\
% \caption{Example of a reflect padding on one tile of the big picture}
%    \label{tiles1}
%\end{figure}
%\noindent

\begin{figure}[H]
    \centering
    \subfigure[Original tile]{\includegraphics[width=0.25\textwidth, height = 0.25\textwidth]{R0C18 (2).png}} 
    \subfigure[Padded tile]{\includegraphics[width=0.25\textwidth, height = 0.25\textwidth]{R0C18.png}} 
    \subfigure[Segmented image]{\includegraphics[width=0.25\textwidth, height = 0.25\textwidth]{mask_R0C18.png}}
    \caption{Process of segmenting the image on the right of figure \ref{imgbig}}
    \label{fig:padding}
\end{figure}

We could finally obtain the segmentation of the image on the right of figure \ref{imgbig}, by merging (in order) all the unpadded masks, obtained by slicing the masks obtained just like the one on the right of figure \ref{fig:padding}. The result of this segmentation problem is shown below.

\begin{figure}[H]
  \centering
  \includegraphics[width=5in]{BigMask0.png}
  \caption[Short caption]{Segmentation of the big picture obtained by merging the $128\times128$ unpadded tiles}
\end{figure}

\subsection{Histological images of cancerous tissues}
As a second application, we downloaded the dataset of the Kaggle challenge \cite{cancerous}. This dataset contained histological images of various cancerous tissues, from kidneys, to livers, to lungs and more. We show below the segmented images obtained through the network on this dataset.
\begin{figure}[H]
    \centering
  \includegraphics[width=5in]{2.jpeg}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[width=5in]{1.jpeg}
      \includegraphics[width=5in]{3.jpeg}
    \caption{Performance of our network on some instances of the dataset \cite{cancerous}, very different from the training one.}
    \label{fig:padding}
\end{figure}
We could not quantify the reliability of the segmentation through the f1 score, due to the absence of a reliable \textit{ground truth} of the images. Let us stress that the images used in this application are way different form the ones used during the training session; despite the little accuracy we therefore considered the solution to the segmentation problem acceptable at glance, meaning that the network has abstracted the problem's global features.


\section{Conclusions}
Let us retrace the steps made during the realization of this project.\\
The aim was to develop a deep neural network able to produce reliable segmentation masks of biomedical images. Starting as a basis from the article \cite{Article} we have manipulated both the Kaggle dataset \cite{dataset} and the neural architecture in order to obtain $256 \times 256$ RGB-images as input and $256 \times 256$ black and white masks as output. The bigger ones have been cropped to create new data images and the smaller symmetrically padded.\\
One of the crucial points of our project resides in the data augmentation made in the dataset class. We have implemented five random transformations that leave the learning process untouched but yield to an infinite number of data and as a consequence the near impossibility of achieving overfitting.\\
The satisfactory 0.93 F1 score reached after ca. 40 epochs of training with the optimal model comes from the solid structure of the net; the pretrained batchnormed vgg16\_bn encoder decomposes and underlines the features necessary to accomplish the segmentation task; while the concatenated decoder grasps the little details lost in the first portion of the model.\\
The collaboration between PyTorch an Optuna allowed us to handle hyperparameter tuning; the results have shown best performances using BCE with logistic loss, coupled with the stochastic gradient descent, as optimizer. We set 0.9 momentum and a warm restart scheduler for the learning rate.\\
As a personal observation, we would like to note the network's ability to underline the segmented blobs' edges, a task that not always the human being manages to accomplish. Maybe this could also explain the reason why we cannot reach a higher F1-score during our experiments, in fact, we noticed that sometimes the target masks were less accurate than the output of our model.


%\bibliographystyle{apacite}
%\bibliography{Ref}
\iffalse
\end{multicols}
\fi
\newpage
\begin{thebibliography}{9}
\bibitem{dataset} 
\texttt{https://www.kaggle.com/c/data-science-bowl-2018}.\\
Kaggle, 2018 Data Science Bowl: Find the nuclei in divergent images to advance medical discovery

\bibitem{lungs} 
Igor Rafael S. Valente, Edson Cavalcanti Neto, Victor Hugo C. Albuquerque.\\
\textit{Automatic 3D pulmonary nodule detection in CT images: A survey}.\\
Computer Methods and Programs in Biomedicine, February 2016.

\bibitem{features} 
Bin Yang, Junjie Yan et al.\\
\textit{Convolutional Channel Features}.\\
ICCV paper, 2015.

\bibitem{Article} 
Olaf Ronneberger, Philipp Fischer, and Thomas Brox\\
\textit{U-Net: Convolutional Networks for Biomedical Image Segmentation}.\\
arXiv, May 2015

\bibitem{cancerous} 
\texttt{https://www.kaggle.com/andrewmvd/cancer-inst-segmentation-and-classification}.\\
Kaggle, 2020: Cancer Instance Segmentation and Classification 1

\end{thebibliography}

\end{document}


